#!/usr/bin/env python3
"""
LLM ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
OpenAI í˜¸í™˜ APIë¥¼ ëŒ€ìƒìœ¼ë¡œ ë¹„ë™ê¸° ë¶€í•˜ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import argparse
import sys

import httpx
import yaml

# GPU ëª¨ë‹ˆí„°ë§ import (ì„ íƒì )
sys.path.insert(0, str(Path(__file__).parent))
try:
    from gpu_monitor import GPUMonitor
    GPU_MONITORING_AVAILABLE = True
except ImportError:
    GPU_MONITORING_AVAILABLE = False


class LLMBenchmark:
    """LLM ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í´ë˜ìŠ¤"""
    
    def __init__(self, config_dir: Path, output_dir: Path, enable_gpu_monitoring: bool = True):
        self.config_dir = config_dir
        self.output_dir = output_dir
        self.results: List[Dict] = []
        self.gpu_monitor: Optional[GPUMonitor] = None
        self.enable_gpu_monitoring = enable_gpu_monitoring and GPU_MONITORING_AVAILABLE
        
        # GPU ëª¨ë‹ˆí„°ë§ ì´ˆê¸°í™”
        if self.enable_gpu_monitoring:
            self.gpu_monitor = GPUMonitor(interval=1.0)
        
    def load_config(self, config_name: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config_path = self.config_dir / f"{config_name}.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    async def send_request(
        self,
        client: httpx.AsyncClient,
        base_url: str,
        api_key: str,
        model: str,
        prompt: str,
        max_tokens: int,
        temperature: float
    ) -> Dict:
        """ë‹¨ì¼ ìš”ì²­ ì „ì†¡ ë° ì§€í‘œ ìˆ˜ì§‘"""
        request_id = f"{int(time.time() * 1000000)}"
        start_time = time.time()
        ttft = None
        tokens_generated = 0
        
        try:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": True
            }
            
            async with client.stream(
                "POST",
                f"{base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=300.0
            ) as response:
                response.raise_for_status()
                
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data.strip() == "[DONE]":
                            break
                        
                        try:
                            chunk = json.loads(data)
                            if ttft is None:
                                ttft = time.time() - start_time
                            
                            # í† í° ì¹´ìš´íŠ¸ (ì‹¤ì œë¡œëŠ” chunkì—ì„œ ì¶”ì¶œ)
                            tokens_generated += 1
                            
                        except json.JSONDecodeError:
                            continue
            
            end_time = time.time()
            total_time = end_time - start_time
            
            return {
                "request_id": request_id,
                "timestamp": datetime.now().isoformat(),
                "success": True,
                "ttft": ttft,
                "total_time": total_time,
                "tokens_generated": tokens_generated,
                "tokens_per_sec": tokens_generated / total_time if total_time > 0 else 0,
                "prompt_length": len(prompt),
                "error": None
            }
            
        except Exception as e:
            return {
                "request_id": request_id,
                "timestamp": datetime.now().isoformat(),
                "success": False,
                "ttft": None,
                "total_time": time.time() - start_time,
                "tokens_generated": 0,
                "tokens_per_sec": 0,
                "prompt_length": len(prompt),
                "error": str(e)
            }
    
    async def run_workload(
        self,
        target: Dict,
        model: str,
        workload: Dict,
        prompts: List[str]
    ):
        """ì›Œí¬ë¡œë“œ ì‹¤í–‰ (ë™ì‹œì„± ì§€ì›, ì§„í–‰ ìƒí™© í‘œì‹œ ê°œì„ , GPU ëª¨ë‹ˆí„°ë§)"""
        print(f"\n=== Starting workload: {workload['name']} ===")
        print(f"Target: {target['name']}, Model: {model}")
        print(f"Duration: {workload['duration']}s, RPS: {workload['rps']}, Concurrency: {workload['concurrency']}")
        
        total_requests = workload['duration'] * workload['rps']
        print(f"Expected total requests: {total_requests}")
        
        # GPU ëª¨ë‹ˆí„°ë§ ì‹œì‘
        if self.gpu_monitor:
            self.gpu_monitor.start()
        
        print(f"\n{'='*70}")
        
        try:
            async with httpx.AsyncClient() as client:
                start_time = time.time()
                request_interval = 1.0 / workload['rps']
                tasks = []
                request_count = 0
                last_print_time = start_time
                
                # ìš”ì²­ ìƒì„± ë£¨í”„
                while time.time() - start_time < workload['duration']:
                    # í”„ë¡¬í”„íŠ¸ ì„ íƒ (ë¼ìš´ë“œ ë¡œë¹ˆ)
                    prompt = prompts[request_count % len(prompts)]
                    
                    # ë¹„ë™ê¸° ìš”ì²­ íƒœìŠ¤í¬ ìƒì„±
                    task = asyncio.create_task(
                        self._send_and_record(
                            client,
                            target,
                            model,
                            prompt,
                            workload
                        )
                    )
                    tasks.append(task)
                    request_count += 1
                    
                    # ì§„í–‰ ìƒí™© ì¶œë ¥ (1ì´ˆë§ˆë‹¤)
                    current_time = time.time()
                    if current_time - last_print_time >= 1.0:
                        elapsed = current_time - start_time
                        remaining = workload['duration'] - elapsed
                        progress = (elapsed / workload['duration']) * 100
                        
                        # GPU ìƒíƒœ ì¶œë ¥
                        if self.gpu_monitor:
                            self.gpu_monitor.print_current_status()
                            print()  # ì¤„ë°”ê¿ˆ
                        
                        print(f"\râ±ï¸  ì§„í–‰: {int(elapsed)}s / {workload['duration']}s "
                              f"({progress:.1f}%) | "
                              f"ìš”ì²­: {request_count:,} / {total_requests:,} | "
                              f"ë‚¨ì€ ì‹œê°„: {int(remaining)}s ({int(remaining/60)}ë¶„ {int(remaining%60)}ì´ˆ)", 
                              end='', flush=True)
                        last_print_time = current_time
                    
                    # RPS ìœ ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
                    await asyncio.sleep(request_interval)
                
                # ìµœì¢… ì§„í–‰ ìƒí™©
                print(f"\râ±ï¸  ìš”ì²­ ìƒì„± ì™„ë£Œ: {request_count:,}ê°œ (100%)                                    ")
                
                # ëª¨ë“  ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°
                print(f"\n{'='*70}")
                print(f"â³ ëª¨ë“  ìš”ì²­ ì‘ë‹µ ëŒ€ê¸° ì¤‘... ({len(tasks):,}ê°œ)")
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for result in results:
                    if isinstance(result, dict):
                        self.results.append(result)
                
                # ìµœì¢… í†µê³„
                success_count = sum(1 for r in self.results if r['success'])
                print(f"âœ… ì™„ë£Œ: {len(self.results):,}ê°œ, ì„±ê³µ: {success_count:,}ê°œ ({success_count/len(self.results)*100:.1f}%)")
                print(f"{'='*70}\n")
                
        finally:
            # GPU ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            if self.gpu_monitor:
                self.gpu_monitor.stop()
    
    async def _send_and_record(
        self,
        client: httpx.AsyncClient,
        target: Dict,
        model: str,
        prompt: str,
        workload: Dict
    ) -> Dict:
        """ìš”ì²­ ì „ì†¡ ë° ê²°ê³¼ ê¸°ë¡"""
        result = await self.send_request(
            client,
            target['base_url'],
            target['api_key'],
            model,
            prompt,
            workload['max_tokens'],
            workload['temperature']
        )
        
        result['workload'] = workload['name']
        result['model'] = model
        result['target'] = target['name']
        
        return result
    
    def save_results(self, output_file: Path):
        """ê²°ê³¼ë¥¼ JSONLê³¼ CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥ (GPU í†µê³„ í¬í•¨)"""
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # JSONL ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            for result in self.results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        
        print(f"\nâœ“ Results saved to: {output_file}")
        print(f"  Total requests: {len(self.results)}")
        success_count = sum(1 for r in self.results if r['success'])
        print(f"  Successful: {success_count} ({success_count/len(self.results)*100:.1f}%)")
        
        # CSV ì €ì¥
        csv_file = output_file.with_suffix('.csv')
        try:
            import pandas as pd
            df = pd.DataFrame(self.results)
            df.to_csv(csv_file, index=False, encoding='utf-8')
            print(f"âœ“ CSV saved to: {csv_file}")
        except Exception as e:
            print(f"âš ï¸  CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # GPU í†µê³„ ì €ì¥
        if self.gpu_monitor and self.gpu_monitor.snapshots:
            gpu_stats = self.gpu_monitor.get_statistics()
            if gpu_stats:
                gpu_file = output_file.parent / f"{output_file.stem}_gpu_stats.json"
                with open(gpu_file, 'w', encoding='utf-8') as f:
                    json.dump(gpu_stats, f, indent=2, ensure_ascii=False)
                print(f"âœ“ GPU stats saved to: {gpu_file}")
                
                # GPU í†µê³„ ìš”ì•½ ì¶œë ¥
                print(f"\nğŸ® GPU ì‚¬ìš©ë¥  ìš”ì•½:")
                for gpu_key, stats in gpu_stats.items():
                    print(f"  {gpu_key.upper()}: "
                          f"í‰ê·  {stats['avg_utilization']:.1f}% | "
                          f"ë©”ëª¨ë¦¬ {stats['avg_memory_used']:.1f}GB | "
                          f"ì „ë ¥ {stats['avg_power']:.0f}W | "
                          f"ì˜¨ë„ {stats['max_temperature']}Â°C")


async def main():
    parser = argparse.ArgumentParser(description="LLM Benchmark Runner")
    parser.add_argument("--target", default="localhost", help="Target name from targets.yaml")
    parser.add_argument("--model", required=True, help="Model name (LiteLLMì—ì„œ ê°€ë™ ì¤‘ì¸ ëª¨ë¸)")
    parser.add_argument("--workload", default="medium-load", help="Workload name from workloads.yaml")
    parser.add_argument("--config-dir", type=Path, default=Path("configs"), help="Config directory")
    parser.add_argument("--output-dir", type=Path, default=Path("results/raw"), help="Output directory")
    
    args = parser.parse_args()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = LLMBenchmark(args.config_dir, args.output_dir)
    
    # ì„¤ì • ë¡œë“œ
    targets_config = benchmark.load_config("targets")
    models_config = benchmark.load_config("models")
    workloads_config = benchmark.load_config("workloads")
    
    # ëŒ€ìƒ ì°¾ê¸°
    target = next((t for t in targets_config['targets'] if t['name'] == args.target), None)
    if not target:
        print(f"Error: Target '{args.target}' not found")
        return
    
    # ëª¨ë¸ ì°¾ê¸°
    model_info = next((m for m in models_config['models'] if m['name'] == args.model), None)
    if not model_info:
        print(f"Error: Model '{args.model}' not found")
        return
    
    # ì›Œí¬ë¡œë“œ ì°¾ê¸°
    workload = next((w for w in workloads_config['workloads'] if w['name'] == args.workload), None)
    if not workload:
        print(f"Error: Workload '{args.workload}' not found")
        return
    
    # í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (difficulty ê¸°ë°˜)
    difficulty = workload.get('difficulty', workload.get('prompt_type', 'medium'))
    prompts = workloads_config['prompt_templates'][difficulty]
    
    # ì›Œí¬ë¡œë“œ ì‹¤í–‰
    await benchmark.run_workload(target, model_info['full_name'], workload, prompts)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = args.output_dir / f"bench_{args.target}_{args.model}_{args.workload}_{timestamp}.jsonl"
    benchmark.save_results(output_file)


if __name__ == "__main__":
    asyncio.run(main())
