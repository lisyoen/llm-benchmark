#!/usr/bin/env python3
"""
ëŒ€í™”í˜• LLM ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ê¸°ë³¸ê°’ìœ¼ë¡œ ì—”í„°ë§Œ ì¹˜ë©´ 5ë¶„ ê³ ë¶€í•˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

CLI íŒŒë¼ë¯¸í„° ì§€ì›:
  --target: ëŒ€ìƒ ì„œë²„ ì´ë¦„
  --model: ëª¨ë¸ ì´ë¦„
  --workload: ì›Œí¬ë¡œë“œ ì´ë¦„
  --duration: í…ŒìŠ¤íŠ¸ ì‹œê°„ (ì´ˆ)
  --rps: ì´ˆë‹¹ ìš”ì²­ ìˆ˜
  --concurrency: ë™ì‹œ ìš”ì²­ ìˆ˜
  --max-tokens: ìµœëŒ€ í† í° ìˆ˜
"""

import asyncio
import sys
import argparse
import subprocess
from pathlib import Path
from datetime import datetime

import httpx
import yaml

# run_bench ëª¨ë“ˆ import
sys.path.insert(0, str(Path(__file__).parent))
from run_bench import LLMBenchmark


def print_header():
    """í—¤ë” ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸš€ LLM ë²¤ì¹˜ë§ˆí¬ ëŒ€í™”í˜• ì‹¤í–‰")
    print("="*60)
    print("\nğŸ’¡ íŒ: ì—”í„°ë§Œ ì¹˜ë©´ ê¸°ë³¸ê°’ ì‚¬ìš© (5ë¶„ ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸)\n")


def load_configs(config_dir: Path):
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(config_dir / "targets.yaml", 'r', encoding='utf-8') as f:
        targets = yaml.safe_load(f)
    with open(config_dir / "models.yaml", 'r', encoding='utf-8') as f:
        models = yaml.safe_load(f)
    with open(config_dir / "workloads.yaml", 'r', encoding='utf-8') as f:
        workloads = yaml.safe_load(f)
    return targets, models, workloads


async def fetch_litellm_models(base_url: str, api_key: str) -> list:
    """LiteLLMì—ì„œ ì‹¤ì œ ê°€ë™ ì¤‘ì¸ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
    
    Args:
        base_url: LiteLLM API base URL (ì˜ˆ: http://localhost:4000/v1)
        api_key: API ì¸ì¦ í‚¤
        
    Returns:
        ëª¨ë¸ ID ë¦¬ìŠ¤íŠ¸. ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            response = await client.get(f"{base_url}/models", headers=headers)
            response.raise_for_status()
            
            data = response.json()
            if 'data' in data:
                model_ids = [model.get('id') for model in data['data'] if model.get('id')]
                return model_ids
            return []
            
    except Exception as e:
        print(f"âš ï¸  LiteLLM ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []


def select_option(prompt: str, options: list, default_index: int = 0) -> tuple:
    """ì˜µì…˜ ì„ íƒ (ê¸°ë³¸ê°’ ì§€ì›)"""
    print(f"\n{prompt}")
    for i, opt in enumerate(options):
        prefix = "â†’" if i == default_index else " "
        print(f"  {prefix} {i+1}. {opt['display']}")
    
    default_display = f"ê¸°ë³¸ê°’: {default_index + 1}"
    choice = input(f"\nì„ íƒ (1-{len(options)}) [{default_display}]: ").strip()
    
    if not choice:
        return default_index, options[default_index]
    
    try:
        idx = int(choice) - 1
        if 0 <= idx < len(options):
            return idx, options[idx]
        else:
            print(f"âš ï¸  ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
            return default_index, options[default_index]
    except ValueError:
        print(f"âš ï¸  ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©")
        return default_index, options[default_index]


def input_with_default(prompt: str, default: any, value_type=str) -> any:
    """ê¸°ë³¸ê°’ì´ ìˆëŠ” ì…ë ¥"""
    user_input = input(f"{prompt} [ê¸°ë³¸ê°’: {default}]: ").strip()
    if not user_input:
        return default
    try:
        return value_type(user_input)
    except:
        print(f"âš ï¸  ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©: {default}")
        return default


def parse_arguments():
    """CLI ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="LLM ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ëŒ€í™”í˜• ë˜ëŠ” CLI ëª¨ë“œ)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ëŒ€í™”í˜• ëª¨ë“œ (ê¸°ë³¸ê°’)
  python3 run_bench_interactive.py
  
  # CLI ëª¨ë“œ - ê¸°ë³¸ ì›Œí¬ë¡œë“œ
  python3 run_bench_interactive.py --target spark-test --model qwen3-coder-30b --workload high-load
  
  # CLI ëª¨ë“œ - ì»¤ìŠ¤í…€ ì„¤ì •
  python3 run_bench_interactive.py --target spark-test --model qwen3-coder-30b --duration 600 --rps 50 --concurrency 100
        """
    )
    
    parser.add_argument("--target", help="ëŒ€ìƒ ì„œë²„ ì´ë¦„")
    parser.add_argument("--model", help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--workload", help="ì›Œí¬ë¡œë“œ ì´ë¦„")
    parser.add_argument("--duration", type=int, help="í…ŒìŠ¤íŠ¸ ì‹œê°„ (ì´ˆ)")
    parser.add_argument("--rps", type=int, help="ì´ˆë‹¹ ìš”ì²­ ìˆ˜")
    parser.add_argument("--concurrency", type=int, help="ë™ì‹œ ìš”ì²­ ìˆ˜")
    parser.add_argument("--max-tokens", type=int, help="ìµœëŒ€ í† í° ìˆ˜")
    parser.add_argument("--temperature", type=float, help="Temperature (0.0-2.0)")
    parser.add_argument("--prompt-type", choices=['short', 'medium', 'long'], help="í”„ë¡¬í”„íŠ¸ íƒ€ì…")
    
    return parser.parse_args()


async def run_with_cli_args(args, config_dir: Path, output_dir: Path):
    """CLI ì¸ìˆ˜ë¡œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    targets, models, workloads = load_configs(config_dir)
    
    # ëŒ€ìƒ ì„œë²„ ì°¾ê¸°
    target = next((t for t in targets['targets'] if t['name'] == args.target), None)
    if not target:
        print(f"âŒ ì˜¤ë¥˜: ëŒ€ìƒ ì„œë²„ '{args.target}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„: {', '.join(t['name'] for t in targets['targets'])}")
        sys.exit(1)
    
    # ëª¨ë¸ ì°¾ê¸°
    model_info = next((m for m in models['models'] if m['name'] == args.model), None)
    if not model_info:
        print(f"âŒ ì˜¤ë¥˜: ëª¨ë¸ '{args.model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(m['name'] for m in models['models'])}")
        sys.exit(1)
    
    # ì›Œí¬ë¡œë“œ ì„¤ì •
    if args.workload:
        # ê¸°ì¡´ ì›Œí¬ë¡œë“œ ì‚¬ìš©
        workload = next((w for w in workloads['workloads'] if w['name'] == args.workload), None)
        if not workload:
            print(f"âŒ ì˜¤ë¥˜: ì›Œí¬ë¡œë“œ '{args.workload}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì›Œí¬ë¡œë“œ: {', '.join(w['name'] for w in workloads['workloads'])}")
            sys.exit(1)
        workload = workload.copy()
    else:
        # ì»¤ìŠ¤í…€ ì›Œí¬ë¡œë“œ ìƒì„±
        if not args.duration or not args.rps:
            print("âŒ ì˜¤ë¥˜: ì›Œí¬ë¡œë“œ ì´ë¦„ ë˜ëŠ” duration/rpsë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        workload = {
            'name': 'custom',
            'description': 'CLI ì»¤ìŠ¤í…€ ì›Œí¬ë¡œë“œ',
            'duration': args.duration,
            'rps': args.rps,
            'concurrency': args.concurrency or 50,
            'max_tokens': args.max_tokens or 2048,
            'temperature': args.temperature or 0.7,
            'prompt_type': args.prompt_type or 'medium'
        }
    
    # CLI ì¸ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ
    if args.duration:
        workload['duration'] = args.duration
    if args.rps:
        workload['rps'] = args.rps
    if args.concurrency:
        workload['concurrency'] = args.concurrency
    if args.max_tokens:
        workload['max_tokens'] = args.max_tokens
    if args.temperature:
        workload['temperature'] = args.temperature
    if args.prompt_type:
        workload['prompt_type'] = args.prompt_type
    
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompts = workloads['prompt_templates'][workload['prompt_type']]
    
    # ì„¤ì • í™•ì¸
    print("\n" + "="*60)
    print("ğŸš€ CLI ëª¨ë“œë¡œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    print("="*60)
    print(f"  ì„œë²„: {target['name']} - {target['description']}")
    print(f"  ëª¨ë¸: {model_info['full_name']}")
    print(f"  ì›Œí¬ë¡œë“œ: {workload.get('description', 'Custom')}")
    print(f"    - ì‹œê°„: {workload['duration']}ì´ˆ ({workload['duration']//60}ë¶„)")
    print(f"    - RPS: {workload['rps']} (ì´ˆë‹¹ ìš”ì²­ ìˆ˜)")
    print(f"    - ë™ì‹œì„±: {workload['concurrency']}")
    print(f"    - ì˜ˆìƒ ì´ ìš”ì²­: {workload['duration'] * workload['rps']}ê°œ")
    print(f"    - ìµœëŒ€ í† í°: {workload['max_tokens']}")
    print(f"    - Temperature: {workload['temperature']}")
    print(f"    - í”„ë¡¬í”„íŠ¸ íƒ€ì…: {workload['prompt_type']}")
    print("="*60 + "\n")
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = LLMBenchmark(config_dir, output_dir)
    
    await benchmark.run_workload(
        target,
        model_info['full_name'],
        workload,
        prompts
    )
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"bench_{target['name']}_{model_info['name']}_{workload['name']}_{timestamp}.jsonl"
    benchmark.save_results(output_file)
    
    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"ï¿½ ì›ì‹œ ë°ì´í„°: {output_file}")
    
    # ìë™ìœ¼ë¡œ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
    print("\nğŸ“Š ê²°ê³¼ ë¶„ì„ ì¤‘...")
    generate_report(output_file)


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    config_dir = Path(__file__).parent.parent / "configs"
    output_dir = Path(__file__).parent.parent / "results" / "raw"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # CLI ì¸ìˆ˜ íŒŒì‹±
    args = parse_arguments()
    
    # CLI ëª¨ë“œ vs ëŒ€í™”í˜• ëª¨ë“œ
    if args.target or args.model or args.workload or args.duration:
        # CLI ëª¨ë“œ: ì¸ìˆ˜ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ CLI ëª¨ë“œ
        await run_with_cli_args(args, config_dir, output_dir)
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        await run_interactive(config_dir, output_dir)


async def run_interactive(config_dir: Path, output_dir: Path):
    """ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰"""
    print_header()
    
    targets, models, workloads = load_configs(config_dir)
    
    # 1. ì„œë²„ ì„ íƒ
    target_options = [
        {
            'name': t['name'],
            'data': t,
            'display': f"{t['name']}: {t['description']}"
        }
        for t in targets['targets']
    ]
    
    # Sparkë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ (ì¸ë±ìŠ¤ ì°¾ê¸°)
    default_target_idx = next(
        (i for i, t in enumerate(target_options) if 'spark' in t['name'].lower()),
        0
    )
    
    _, selected_target = select_option(
        "ğŸ“¡ ë²¤ì¹˜ë§ˆí¬ ëŒ€ìƒ ì„œë²„ ì„ íƒ:",
        target_options,
        default_target_idx
    )
    target = selected_target['data']
    
    # 2. ëª¨ë¸ ì„ íƒ - LiteLLMì—ì„œ ì‹¤ì œ ê°€ë™ ì¤‘ì¸ ëª¨ë¸ ì¡°íšŒ
    print("\nğŸ” LiteLLMì—ì„œ ê°€ë™ ì¤‘ì¸ ëª¨ë¸ ì¡°íšŒ ì¤‘...")
    available_models = await fetch_litellm_models(target['base_url'], target['api_key'])
    
    if available_models:
        # LiteLLM APIë¡œë¶€í„° ëª¨ë¸ ëª©ë¡ì„ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜¨ ê²½ìš°
        print(f"âœ… {len(available_models)}ê°œì˜ ëª¨ë¸ì´ ê°€ë™ ì¤‘ì…ë‹ˆë‹¤.\n")
        
        model_options = []
        for model_id in available_models:
            # ëª¨ë¸ IDì—ì„œ ê°„ë‹¨í•œ í‘œì‹œ ì´ë¦„ ìƒì„±
            display_name = model_id
            if '/' in model_id:
                display_name = model_id.split('/')[-1]
            
            model_options.append({
                'name': model_id,
                'data': {
                    'name': model_id.replace('/', '-'),  # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡
                    'full_name': model_id,
                    'description': f'LiteLLM ê°€ë™ ëª¨ë¸'
                },
                'display': f"{display_name} ({model_id})"
            })
        
        # ì²« ë²ˆì§¸ ëª¨ë¸ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ
        default_model_idx = 0
        
    else:
        # LiteLLM API ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ models.yaml ì‚¬ìš©
        print("âš ï¸  LiteLLM ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. configs/models.yaml ì‚¬ìš©\n")
        
        model_options = [
            {
                'name': m['name'],
                'data': m,
                'display': f"{m['name']}: {m['description']}"
            }
            for m in models['models']
        ]
        
        # qwen3-coder-30bë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ
        default_model_idx = next(
            (i for i, m in enumerate(model_options) if 'qwen3-coder-30b' in m['name']),
            0
        )
    
    _, selected_model = select_option(
        "ğŸ¤– í…ŒìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ:",
        model_options,
        default_model_idx
    )
    model_info = selected_model['data']
    
    # 3. ì›Œí¬ë¡œë“œ ì„ íƒ ë˜ëŠ” ì»¤ìŠ¤í…€
    print("\nâš™ï¸  ì›Œí¬ë¡œë“œ ì„¤ì •:")
    print("  â†’ 1. ê¸°ë³¸ ì„¤ì • ì‚¬ìš© (5ë¶„ ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸)")
    print("    2. ì‚¬ì „ ì •ì˜ëœ ì›Œí¬ë¡œë“œ ì„ íƒ")
    print("    3. ì»¤ìŠ¤í…€ ì„¤ì •")
    
    workload_choice = input("\nì„ íƒ (1-3) [ê¸°ë³¸ê°’: 1]: ").strip()
    
    if workload_choice == "3":
        # ì»¤ìŠ¤í…€ ì„¤ì •
        print("\nğŸ“ ì»¤ìŠ¤í…€ ì›Œí¬ë¡œë“œ ì„¤ì •:")
        duration = input_with_default("  í…ŒìŠ¤íŠ¸ ì‹œê°„ (ì´ˆ)", 300, int)
        rps = input_with_default("  ì´ˆë‹¹ ìš”ì²­ ìˆ˜ (RPS)", 20, int)
        max_tokens = input_with_default("  ìµœëŒ€ í† í° ìˆ˜", 1024, int)
        temperature = input_with_default("  Temperature", 0.7, float)
        
        prompt_types = ["short", "medium", "long"]
        print("\n  í”„ë¡¬í”„íŠ¸ ê¸¸ì´:")
        for i, pt in enumerate(prompt_types):
            prefix = "â†’" if i == 1 else " "
            print(f"    {prefix} {i+1}. {pt}")
        prompt_choice = input(f"  ì„ íƒ (1-3) [ê¸°ë³¸ê°’: 2 (medium)]: ").strip()
        
        if not prompt_choice or prompt_choice == "2":
            prompt_type = "medium"
        elif prompt_choice == "1":
            prompt_type = "short"
        elif prompt_choice == "3":
            prompt_type = "long"
        else:
            prompt_type = "medium"
        
        workload = {
            'name': 'custom',
            'description': f'ì»¤ìŠ¤í…€ ì„¤ì • ({duration}ì´ˆ, RPS:{rps})',
            'duration': duration,
            'rps': rps,
            'concurrency': min(rps * 10, 100),  # RPSì˜ 10ë°° ë˜ëŠ” ìµœëŒ€ 100
            'max_tokens': max_tokens,
            'temperature': temperature,
            'prompt_type': prompt_type
        }
        
    elif workload_choice == "2":
        # ì‚¬ì „ ì •ì˜ëœ ì›Œí¬ë¡œë“œ
        workload_options = [
            {
                'name': w['name'],
                'data': w,
                'display': f"{w['name']}: {w['description']} ({w['duration']}ì´ˆ, RPS:{w['rps']})"
            }
            for w in workloads['workloads']
        ]
        
        # high-loadë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ
        default_workload_idx = next(
            (i for i, w in enumerate(workload_options) if 'high' in w['name']),
            1
        )
        
        _, selected_workload = select_option(
            "ì›Œí¬ë¡œë“œ ì„ íƒ:",
            workload_options,
            default_workload_idx
        )
        workload = selected_workload['data']
        
    else:
        # ê¸°ë³¸ê°’: 5ë¶„ ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸
        workload = {
            'name': 'high-load-5min',
            'description': '5ë¶„ ê³ ë¶€í•˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸',
            'duration': 300,
            'rps': 20,
            'concurrency': 50,
            'max_tokens': 1024,
            'temperature': 0.7,
            'prompt_type': 'medium'
        }
    
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompts = workloads['prompt_templates'][workload['prompt_type']]
    
    # ì„¤ì • í™•ì¸
    print("\n" + "="*60)
    print("ğŸ“‹ ë²¤ì¹˜ë§ˆí¬ ì„¤ì • í™•ì¸")
    print("="*60)
    print(f"  ì„œë²„: {target['name']} - {target['description']}")
    print(f"  ëª¨ë¸: {model_info['full_name']}")
    print(f"  ì›Œí¬ë¡œë“œ: {workload['description']}")
    print(f"    - ì‹œê°„: {workload['duration']}ì´ˆ ({workload['duration']//60}ë¶„)")
    print(f"    - RPS: {workload['rps']} (ì´ˆë‹¹ ìš”ì²­ ìˆ˜)")
    print(f"    - ë™ì‹œì„±: {workload['concurrency']}")
    print(f"    - ì˜ˆìƒ ì´ ìš”ì²­: {workload['duration'] * workload['rps']}ê°œ")
    print(f"    - ìµœëŒ€ í† í°: {workload['max_tokens']}")
    print(f"    - Temperature: {workload['temperature']}")
    print(f"    - í”„ë¡¬í”„íŠ¸ íƒ€ì…: {workload['prompt_type']}")
    print("="*60)
    
    confirm = input("\nì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n) [ê¸°ë³¸ê°’: Y]: ").strip().lower()
    if confirm and confirm != 'y':
        print("\nâŒ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    print("\nğŸš€ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘!\n")
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = LLMBenchmark(config_dir, output_dir)
    
    await benchmark.run_workload(
        target,
        model_info['full_name'],
        workload,
        prompts
    )
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"bench_{target['name']}_{model_info['name']}_{workload['name']}_{timestamp}.jsonl"
    benchmark.save_results(output_file)
    
    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print(f"ğŸ“ ì›ì‹œ ë°ì´í„°: {output_file}")
    
    # ìë™ìœ¼ë¡œ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
    print("\nğŸ“Š ê²°ê³¼ ë¶„ì„ ì¤‘...")
    generate_report(output_file)


def generate_report(result_file: Path):
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„ ë° ë³´ê³ ì„œ ìë™ ìƒì„±"""
    project_root = result_file.parent.parent.parent
    scripts_dir = project_root / "scripts"
    summary_dir = project_root / "results" / "summary"
    reports_dir = project_root / "results" / "reports"
    
    summary_dir.mkdir(parents=True, exist_ok=True)
    reports_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # 1. í†µê³„ ë¶„ì„ (parse_metrics.py)
        print("  â†’ í†µê³„ ê³„ì‚° ì¤‘...")
        result = subprocess.run(
            [sys.executable, str(scripts_dir / "parse_metrics.py"), str(result_file)],
            capture_output=True,
            text=True,
            check=True
        )
        print(result.stdout)
        
        # 2. ë³´ê³ ì„œ ìƒì„± (gen_report.py)
        print("  â†’ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        result = subprocess.run(
            [sys.executable, str(scripts_dir / "gen_report.py")],
            capture_output=True,
            text=True,
            check=True
        )
        print(result.stdout)
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        # ê°€ì¥ ìµœê·¼ ë³´ê³ ì„œ ì°¾ê¸°
        report_files = sorted(reports_dir.glob("benchmark_report_*.md"), key=lambda p: p.stat().st_mtime, reverse=True)
        if not report_files:
            # êµ¬í˜• íŒŒì¼ëª…ë„ í™•ì¸
            report_files = list(reports_dir.glob("benchmark_report.md"))
        
        if report_files:
            report_file = report_files[0]
            print(f"\nâœ¨ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
            print(f"ğŸ“„ ë³´ê³ ì„œ: {report_file}")
            
            # CSV íŒŒì¼ ì°¾ê¸°
            csv_files = list(summary_dir.glob("*.csv"))
            if csv_files:
                latest_csv = max(csv_files, key=lambda p: p.stat().st_mtime)
                print(f"ğŸ“Š í†µê³„ ìš”ì•½: {latest_csv}")
        
    except subprocess.CalledProcessError as e:
        print(f"\nâš ï¸  ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ:")
        print(e.stderr)
        print(f"\nìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:")
        print(f"  python3 scripts/parse_metrics.py {result_file}")
        print(f"  python3 scripts/gen_report.py")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
